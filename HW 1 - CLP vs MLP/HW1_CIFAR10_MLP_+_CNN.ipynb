{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1 : CIFAR10 MLP + CNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efgB-7mO3HJ_",
        "colab_type": "text"
      },
      "source": [
        "# MLP\n",
        "- For the MLP I used ReLU for the activation function and a 0.2 dropout for the regularization.\n",
        "- For the optimizer, I tried using ADAM instead of SGD but since I got a lower accuracy with ADAM, I continued using SGD instead.\n",
        "> 256 hidden layers, 20 epochs\n",
        "> * **Adam** - 44.8%\n",
        "> * **SGD** - 48.0%\n",
        "\n",
        "- I also tested using 256, 512, or 1024 hidden layers. Each of them have close results but I chose 512 layers as a compromise to get a higher accuracy with lower training time.\n",
        "> SGD optimizer, 20 epochs\n",
        "> * **256** - 48.0%\n",
        "> * **512** - 48.9%\n",
        "> * **1024** - 50%\n",
        "\n",
        "- I also adjusted the number of epochs and found that using 100 epochs is enough to train the model without overfitting\n",
        "> 512 hidden layers, SGD optimizer\n",
        "> * **20** - 48.9%\n",
        "> * **30** - 51.1%\n",
        "> * **50** - 52.3%\n",
        "> * **100** - 55.0%\n",
        "\n",
        "- The final parameters I used are **512** hidden units, **SGD** optimizer, and **100** epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKTM92y8re_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea55af20-e4ce-45cc-d4a4-4198f1416aba"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# compute the number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# convert to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# change the shape from a 2d image (assumed square) with 3 channels\n",
        "image_size = x_train.shape[1]\n",
        "image_channels = x_train.shape[3]\n",
        "input_size = image_size * image_size * image_channels\n",
        "\n",
        "# reshape to a vector\n",
        "x_train = np.reshape(x_train, [-1, input_size])\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "\n",
        "# change datatype to float32\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "batch_size = 128\n",
        "hidden_units = 512\n",
        "dropout = 0.2\n",
        "\n",
        "# Create a 3-layer MLP with ReLU and dropout regularization\n",
        "model = Sequential()\n",
        "\n",
        "# layer 1\n",
        "model.add(Dense(hidden_units, input_dim = input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "# layer 2\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# create the loss function for a one-hot vector\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='sgd',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# train network\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=batch_size)\n",
        "\n",
        "# use test data to validate\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"\\n ACCURACY: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,841,162\n",
            "Trainable params: 1,841,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 14s 282us/step - loss: 2.0456 - acc: 0.2568\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.8655 - acc: 0.3346\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.7963 - acc: 0.3633\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.7461 - acc: 0.3852\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.7090 - acc: 0.3953\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.6788 - acc: 0.4064\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.6557 - acc: 0.4161\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.6307 - acc: 0.4242\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.6103 - acc: 0.4325\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.5912 - acc: 0.4390\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.5761 - acc: 0.4437\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.5581 - acc: 0.4499\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.5421 - acc: 0.4573\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.5326 - acc: 0.4593\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.5193 - acc: 0.4651\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.5045 - acc: 0.4692\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.4950 - acc: 0.4720\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.4769 - acc: 0.4785\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.4696 - acc: 0.4835\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.4590 - acc: 0.4863\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.4495 - acc: 0.4894\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.4404 - acc: 0.4930\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.4270 - acc: 0.4999\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.4187 - acc: 0.4990\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.4093 - acc: 0.5029\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.4031 - acc: 0.5069\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.3974 - acc: 0.5105\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.3847 - acc: 0.5138\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.3759 - acc: 0.5156\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.3671 - acc: 0.5175\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.3631 - acc: 0.5188\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.3529 - acc: 0.5232\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.3476 - acc: 0.5259\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.3405 - acc: 0.5259\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.3316 - acc: 0.5321\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.3239 - acc: 0.5339\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.3194 - acc: 0.5363\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.3135 - acc: 0.5378\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.3055 - acc: 0.5386\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.2970 - acc: 0.5448\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.2902 - acc: 0.5437\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.2841 - acc: 0.5472\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.2794 - acc: 0.5489\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.2755 - acc: 0.5487\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.2691 - acc: 0.5537\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.2599 - acc: 0.5548\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.2544 - acc: 0.5594\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.2483 - acc: 0.5581\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.2428 - acc: 0.5621\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.2375 - acc: 0.5642\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.2326 - acc: 0.5669\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.2269 - acc: 0.5676\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.2198 - acc: 0.5717\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.2140 - acc: 0.5716\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.2094 - acc: 0.5734\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.2074 - acc: 0.5748\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.1993 - acc: 0.5779\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.1941 - acc: 0.5807\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.1865 - acc: 0.5798\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 13s 259us/step - loss: 1.1852 - acc: 0.5815\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.1789 - acc: 0.5855\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.1777 - acc: 0.5843\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.1680 - acc: 0.5892\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.1627 - acc: 0.5892\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.1588 - acc: 0.5902\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 13s 265us/step - loss: 1.1537 - acc: 0.5953\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.1519 - acc: 0.5933\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.1408 - acc: 0.5982\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.1387 - acc: 0.5953\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.1291 - acc: 0.6011\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.1331 - acc: 0.6004\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.1232 - acc: 0.6017\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.1181 - acc: 0.6026\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 13s 260us/step - loss: 1.1160 - acc: 0.6081\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.1079 - acc: 0.6077\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.1053 - acc: 0.6109\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.1007 - acc: 0.6125\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.0960 - acc: 0.6138\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0888 - acc: 0.6171\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.0852 - acc: 0.6180\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 13s 267us/step - loss: 1.0836 - acc: 0.6179\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0755 - acc: 0.6211\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.0745 - acc: 0.6221\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.0671 - acc: 0.6234\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 13s 264us/step - loss: 1.0650 - acc: 0.6243\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.0610 - acc: 0.6232\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0591 - acc: 0.6254\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.0505 - acc: 0.6300\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.0473 - acc: 0.6297\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.0441 - acc: 0.6312\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0344 - acc: 0.6335\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.0365 - acc: 0.6355\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0283 - acc: 0.6370\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0215 - acc: 0.6396\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 13s 265us/step - loss: 1.0195 - acc: 0.6397\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.0197 - acc: 0.6380\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0107 - acc: 0.6440\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 13s 263us/step - loss: 1.0074 - acc: 0.6433\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 1.0066 - acc: 0.6452\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 13s 262us/step - loss: 0.9991 - acc: 0.6477\n",
            "10000/10000 [==============================] - 1s 124us/step\n",
            "\n",
            " ACCURACY: 55.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZXZ6bQOflw",
        "colab_type": "text"
      },
      "source": [
        "# CNN\n",
        "- For the CNN, I essentially tested \n",
        "- For the kernel size, I tried using a 3x3 and 5x5 kernel, but at 10 epochs the 3x3 kernel model is more accurate (but there isn't that much of a difference in accuracy).\n",
        "> Using 64 filters, 10 epochs, adam optimizer, valid padding and a (1,1) stride\n",
        "> - **3x3** - 68.0%\n",
        "> - **5x5** - 66.5%\n",
        "- I then tried using 32 and 64 filters, but it seems that 64 filters already is the better one to use\n",
        "- When it comes to padding, it seems better if the image size is preserved\n",
        "> Using 64 filters, 10 epochs, adam optimizer and a (1,1) stride\n",
        "> - **valid** - 67.10%\n",
        "> - **same** - 70.20%\n",
        "- The final parameters are used are **3x3** kernel size, **64** filters, **10** epochs, **adam** optimizer, **(1,1)** stride, **same** padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGTXtUykObAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "e8ad2fbc-ffa9-423c-90f7-e2f3b2f38f3b"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from keras.utils import to_categorical, plot_model\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# compute the number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# convert to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        " \n",
        "image_size = x_train.shape[1]\n",
        "image_channels = x_train.shape[3]\n",
        "\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, image_channels])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, image_channels])\n",
        "\n",
        "# change datatype\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "input_shape = (image_size, image_size, image_channels)\n",
        "batch_size = 64\n",
        "kernel_size = 3\n",
        "pool_size = 2\n",
        "filters = 64\n",
        "dropout = 0.2\n",
        "\n",
        "# Create 3-layer CNN\n",
        "cnn = Sequential()\n",
        "\n",
        "# layer 1\n",
        "cnn.add(Conv2D(\n",
        "    filters = filters,\n",
        "    kernel_size = kernel_size,\n",
        "    activation = 'relu',\n",
        "    input_shape = input_shape,\n",
        "    padding = 'same'\n",
        "))\n",
        "cnn.add(MaxPooling2D(pool_size))\n",
        "\n",
        "# layer 2\n",
        "cnn.add(Conv2D(\n",
        "    filters = filters,\n",
        "    kernel_size = kernel_size,\n",
        "    activation = 'relu',\n",
        "    padding = 'same'\n",
        "))\n",
        "cnn.add(MaxPooling2D(pool_size))\n",
        "\n",
        "# output layer\n",
        "cnn.add(Conv2D(\n",
        "    filters = filters,\n",
        "    kernel_size = kernel_size,\n",
        "    activation = 'relu',\n",
        "    strides = strides,\n",
        "    padding = 'same'\n",
        "))\n",
        "cnn.add(Flatten())\n",
        "\n",
        "# output should be a 10-dim one-hot vector\n",
        "cnn.add(Dense(num_labels))\n",
        "cnn.add(Dropout(dropout))\n",
        "cnn.add(Activation('softmax'))\n",
        "\n",
        "cnn.summary()\n",
        "\n",
        "# create the loss function for a one-hot vector\n",
        "cnn.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'adam',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# train network\n",
        "cnn.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
        "\n",
        "loss, acc = cnn.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_68 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_69 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_70 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 10)                40970     \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_74 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 116,618\n",
            "Trainable params: 116,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 147s 3ms/step - loss: 1.6397 - acc: 0.4153\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 1.2938 - acc: 0.5417\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 1.1476 - acc: 0.5918\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 1.0535 - acc: 0.6227\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 144s 3ms/step - loss: 0.9839 - acc: 0.6469\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 0.9274 - acc: 0.6651\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 0.8732 - acc: 0.6811\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 0.8347 - acc: 0.6939\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 145s 3ms/step - loss: 0.7936 - acc: 0.7077\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 144s 3ms/step - loss: 0.7615 - acc: 0.7167\n",
            "10000/10000 [==============================] - 9s 916us/step\n",
            "\n",
            "Test accuracy: 72.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkebB6vNudAV",
        "colab_type": "text"
      },
      "source": [
        "# Comparison\n",
        "As we can observe, even with a few training epochs, the CNN model can better classify images from the CIFAR10 dataset compared to the MLP model.\n",
        "\n",
        "The main reason for this is because the CNN model is more suitable for images. This is due to the convolutions using relationship of the data point to other adjacent data points. The CNN can then recognize small patterns from a date set and merge it with other discovered patterns to recognize a larger object (e.g patterns of lines and contours can help identify different objects)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LGAnKTRBZVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
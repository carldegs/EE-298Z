{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pump It UP",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carldegs/EE-298Z/blob/master/Pump%20It%20Up/Pump_It_UP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qsEVPWT4WeT",
        "colab_type": "code",
        "outputId": "2e302255-41d0-4515-b743-02aa5651aa33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction  import FeatureHasher\n",
        "\n",
        "cols_to_remove = ['id', 'wpt_name', 'num_private', 'region_code', 'lga', 'ward', 'subvillage', 'recorded_by', 'scheme_name', 'extraction_type_group', 'extraction_type_class', 'payment', 'quality_group', 'quantity_group', 'source_type', 'source_class', 'waterpoint_type_group']\n",
        "cols_to_normalize = ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'population', 'construction_year']\n",
        "cols_bool_to_int = ['public_meeting', 'permit']\n",
        "cols_to_circularize = ['date_recorded']\n",
        "cols_to_feature_hash = ['funder', 'installer']\n",
        "cols_to_one_hot = ['basin', 'region', 'district_code', 'scheme_management', 'extraction_type', 'management', 'management_group', 'payment_type', 'water_quality', 'quantity', 'source', 'waterpoint_type']\n",
        "\n",
        "FEATURE_HASHER_NUM_FEATURES = 16\n",
        "\n",
        "x_url = \"https://raw.githubusercontent.com/carldegs/EE-298Z/master/Pump%20It%20Up/x_train.csv\"\n",
        "y_url = \"https://raw.githubusercontent.com/carldegs/EE-298Z/master/Pump%20It%20Up/y_train.csv\"\n",
        "x_problem_url = \"https://raw.githubusercontent.com/carldegs/EE-298Z/master/Pump%20It%20Up/x_test.csv\"\n",
        "\n",
        "def drop_and_fill_na(df):\n",
        "    \"Remove columns that will not be used and fill null values with the corresponding alternative values\"\n",
        "    df = df.drop(cols_to_remove, axis=1)\n",
        "    df[cols_to_one_hot + cols_to_feature_hash] = df[cols_to_one_hot + cols_to_feature_hash].fillna('Not Known')\n",
        "    df[cols_bool_to_int] = df[cols_bool_to_int].fillna(False)\n",
        "    return df\n",
        "\n",
        "def transform_data(df):\n",
        "    # For date_recorder, convert to three columns, 2 for cyclical day of year recorded and the year recorded\n",
        "    # https://www.avanwyk.com/encoding-cyclical-features-for-deep-learning/\n",
        "    df[\"date_recorded\"] = pd.to_datetime(df[\"date_recorded\"], format=\"%Y-%m-%d\")\n",
        "    df[\"day_of_year_recorded\"] = df[\"date_recorded\"].apply(lambda x: x.timetuple().tm_yday)\n",
        "\n",
        "    df[\"year_recorded\"] = df[\"date_recorded\"].apply(lambda x: x.year)\n",
        "    df[\"doy_recorded_sin\"] = df[\"day_of_year_recorded\"].apply(lambda x: np.sin(2 * np.pi * x/365.0)) # TODO: how about leap year?\n",
        "    df[\"doy_recorded_cos\"] = df[\"day_of_year_recorded\"].apply(lambda x: np.cos(2 * np.pi * x/365.0)) # TODO: how about leap year?\n",
        "    df = df.drop([\"date_recorded\", \"day_of_year_recorded\"], axis=1)\n",
        "\n",
        "    # One Hot Encode data\n",
        "    oh_enc_data = oh_enc.transform(df[cols_to_one_hot])\n",
        "    oh_enc_cols = oh_enc.get_feature_names()\n",
        "    oh_enc_df = pd.DataFrame(oh_enc_data, columns=oh_enc_cols)\n",
        "    df = df.drop(cols_to_one_hot, axis=1)\n",
        "    df = pd.concat([df, oh_enc_df], axis=1)\n",
        "\n",
        "    # Change boolean to int\n",
        "    df[cols_bool_to_int] = df[cols_bool_to_int].astype(int)\n",
        "\n",
        "    # Use feature hasher\n",
        "    funder_feat_hasher_data = funder_feat_hasher.transform(df['funder']).toarray()\n",
        "    funder_feat_hasher_cols = ['funder_feat_hasher_' + str(i+1) for i in range(FEATURE_HASHER_NUM_FEATURES)]\n",
        "    funder_feat_hasher_df = pd.DataFrame(funder_feat_hasher_data, columns=funder_feat_hasher_cols)\n",
        "\n",
        "    installer_feat_hasher_data = installer_feat_hasher.transform(df['installer']).toarray()\n",
        "    installer_feat_hasher_cols = ['installer_feat_hasher_' + str(j+1) for j in range(FEATURE_HASHER_NUM_FEATURES)]\n",
        "    installer_feat_hasher_df = pd.DataFrame(installer_feat_hasher_data, columns=installer_feat_hasher_cols)\n",
        "\n",
        "    df = df.drop(cols_to_feature_hash, axis=1)\n",
        "    df = pd.concat([df, funder_feat_hasher_df, installer_feat_hasher_df], axis=1)\n",
        "\n",
        "    # normalize integers\n",
        "    for col in df[cols_to_normalize + funder_feat_hasher_cols + installer_feat_hasher_cols + ['year_recorded']]:\n",
        "        df[col] -= df[col].min()\n",
        "        df[col] /= df[col].max()\n",
        "        \n",
        "    return df\n",
        "\n",
        "# fetch data sets\n",
        "x = pd.read_csv(x_url)\n",
        "x_prob = pd.read_csv(x_problem_url)\n",
        "\n",
        "y = pd.read_csv(y_url)\n",
        "y = y.pop('status_group').values\n",
        "\n",
        "# initialize data sets\n",
        "x = drop_and_fill_na(x)\n",
        "x_prob_id = x_prob['id']\n",
        "x_prob = drop_and_fill_na(x_prob)\n",
        "\n",
        "# Fit encoders and transform x\n",
        "oh_enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "oh_enc.fit(x[cols_to_one_hot])\n",
        "\n",
        "y_oh_enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "funder_feat_hasher = FeatureHasher(n_features=FEATURE_HASHER_NUM_FEATURES, input_type='string', alternate_sign=False)\n",
        "funder_feat_hasher.fit(x['funder'])\n",
        "\n",
        "installer_feat_hasher = FeatureHasher(n_features=FEATURE_HASHER_NUM_FEATURES, input_type='string', alternate_sign=False)\n",
        "installer_feat_hasher.fit(x['installer'])\n",
        "\n",
        "x = transform_data(x)\n",
        "x_prob = transform_data(x_prob)\n",
        "y = y_oh_enc.fit_transform(y.reshape(-1,1))\n",
        "\n",
        "# split into train and test data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 27)\n",
        "num_labels = 3\n",
        "input_size = x_train.shape[1]\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# network parameters\n",
        "batch_size = 128\n",
        "hidden_units = 512\n",
        "dropout = 0.3\n",
        "\n",
        "# Create a 3-layer MLP with ReLU and dropout regularization\n",
        "model = Sequential()\n",
        "\n",
        "# layer 1\n",
        "model.add(Dense(hidden_units, input_dim = input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "# layer 2\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# create the loss function for a one-hot vector\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='sgd',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# train network\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=batch_size)\n",
        "\n",
        "# use test data to validate\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"\\n ACCURACY: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 512)               91648     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 1539      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 355,843\n",
            "Trainable params: 355,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "41580/41580 [==============================] - 4s 99us/step - loss: 0.8617 - acc: 0.5966\n",
            "Epoch 2/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.7738 - acc: 0.6737\n",
            "Epoch 3/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.7311 - acc: 0.6944\n",
            "Epoch 4/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.7047 - acc: 0.7065\n",
            "Epoch 5/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.6896 - acc: 0.7117\n",
            "Epoch 6/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6790 - acc: 0.7159\n",
            "Epoch 7/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.6701 - acc: 0.7203\n",
            "Epoch 8/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.6619 - acc: 0.7245\n",
            "Epoch 9/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6564 - acc: 0.7278\n",
            "Epoch 10/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6519 - acc: 0.7304\n",
            "Epoch 11/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.6473 - acc: 0.7307\n",
            "Epoch 12/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6450 - acc: 0.7321\n",
            "Epoch 13/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.6410 - acc: 0.7345\n",
            "Epoch 14/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6374 - acc: 0.7370\n",
            "Epoch 15/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6357 - acc: 0.7358\n",
            "Epoch 16/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6313 - acc: 0.7386\n",
            "Epoch 17/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6283 - acc: 0.7412\n",
            "Epoch 18/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6273 - acc: 0.7419\n",
            "Epoch 19/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6247 - acc: 0.7412\n",
            "Epoch 20/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6210 - acc: 0.7425\n",
            "Epoch 21/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6192 - acc: 0.7452\n",
            "Epoch 22/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6172 - acc: 0.7461\n",
            "Epoch 23/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6155 - acc: 0.7453\n",
            "Epoch 24/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6129 - acc: 0.7477\n",
            "Epoch 25/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.6124 - acc: 0.7484\n",
            "Epoch 26/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6090 - acc: 0.7488\n",
            "Epoch 27/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.6082 - acc: 0.7500\n",
            "Epoch 28/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6064 - acc: 0.7503\n",
            "Epoch 29/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6037 - acc: 0.7507\n",
            "Epoch 30/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6007 - acc: 0.7525\n",
            "Epoch 31/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.6005 - acc: 0.7552\n",
            "Epoch 32/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.6007 - acc: 0.7523\n",
            "Epoch 33/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5971 - acc: 0.7553\n",
            "Epoch 34/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5941 - acc: 0.7559\n",
            "Epoch 35/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5922 - acc: 0.7578\n",
            "Epoch 36/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.5918 - acc: 0.7585\n",
            "Epoch 37/100\n",
            "41580/41580 [==============================] - 4s 99us/step - loss: 0.5927 - acc: 0.7571\n",
            "Epoch 38/100\n",
            "41580/41580 [==============================] - 4s 98us/step - loss: 0.5880 - acc: 0.7591\n",
            "Epoch 39/100\n",
            "41580/41580 [==============================] - 4s 97us/step - loss: 0.5876 - acc: 0.7598\n",
            "Epoch 40/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.5858 - acc: 0.7604\n",
            "Epoch 41/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.5848 - acc: 0.7604\n",
            "Epoch 42/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.5844 - acc: 0.7622\n",
            "Epoch 43/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5839 - acc: 0.7613\n",
            "Epoch 44/100\n",
            "41580/41580 [==============================] - 4s 98us/step - loss: 0.5817 - acc: 0.7615\n",
            "Epoch 45/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.5801 - acc: 0.7623\n",
            "Epoch 46/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5781 - acc: 0.7639\n",
            "Epoch 47/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5777 - acc: 0.7649\n",
            "Epoch 48/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5768 - acc: 0.7650\n",
            "Epoch 49/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5742 - acc: 0.7631\n",
            "Epoch 50/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5731 - acc: 0.7665\n",
            "Epoch 51/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5730 - acc: 0.7650\n",
            "Epoch 52/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5710 - acc: 0.7671\n",
            "Epoch 53/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5706 - acc: 0.7687\n",
            "Epoch 54/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5699 - acc: 0.7672\n",
            "Epoch 55/100\n",
            "41580/41580 [==============================] - 4s 97us/step - loss: 0.5686 - acc: 0.7677\n",
            "Epoch 56/100\n",
            "41580/41580 [==============================] - 4s 96us/step - loss: 0.5673 - acc: 0.7697\n",
            "Epoch 57/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5636 - acc: 0.7663\n",
            "Epoch 58/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5661 - acc: 0.7677\n",
            "Epoch 59/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5622 - acc: 0.7694\n",
            "Epoch 60/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5639 - acc: 0.7703\n",
            "Epoch 61/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5627 - acc: 0.7704\n",
            "Epoch 62/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5598 - acc: 0.7717\n",
            "Epoch 63/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5606 - acc: 0.7718\n",
            "Epoch 64/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5590 - acc: 0.7704\n",
            "Epoch 65/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5582 - acc: 0.7724\n",
            "Epoch 66/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5568 - acc: 0.7717\n",
            "Epoch 67/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5566 - acc: 0.7747\n",
            "Epoch 68/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5570 - acc: 0.7715\n",
            "Epoch 69/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5533 - acc: 0.7737\n",
            "Epoch 70/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5518 - acc: 0.7746\n",
            "Epoch 71/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5526 - acc: 0.7744\n",
            "Epoch 72/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5512 - acc: 0.7728\n",
            "Epoch 73/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5496 - acc: 0.7741\n",
            "Epoch 74/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5492 - acc: 0.7767\n",
            "Epoch 75/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5495 - acc: 0.7762\n",
            "Epoch 76/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5478 - acc: 0.7760\n",
            "Epoch 77/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5453 - acc: 0.7779\n",
            "Epoch 78/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5451 - acc: 0.7767\n",
            "Epoch 79/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5454 - acc: 0.7774\n",
            "Epoch 80/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5467 - acc: 0.7752\n",
            "Epoch 81/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5431 - acc: 0.7778\n",
            "Epoch 82/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5423 - acc: 0.7791\n",
            "Epoch 83/100\n",
            "41580/41580 [==============================] - 4s 92us/step - loss: 0.5429 - acc: 0.7787\n",
            "Epoch 84/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5415 - acc: 0.7791\n",
            "Epoch 85/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5418 - acc: 0.7775\n",
            "Epoch 86/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5388 - acc: 0.7791\n",
            "Epoch 87/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5401 - acc: 0.7785\n",
            "Epoch 88/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5370 - acc: 0.7796\n",
            "Epoch 89/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5372 - acc: 0.7806\n",
            "Epoch 90/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5373 - acc: 0.7793\n",
            "Epoch 91/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5367 - acc: 0.7822\n",
            "Epoch 92/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5331 - acc: 0.7835\n",
            "Epoch 93/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5338 - acc: 0.7808\n",
            "Epoch 94/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5314 - acc: 0.7826\n",
            "Epoch 95/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5342 - acc: 0.7806\n",
            "Epoch 96/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5328 - acc: 0.7815\n",
            "Epoch 97/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5306 - acc: 0.7824\n",
            "Epoch 98/100\n",
            "41580/41580 [==============================] - 4s 95us/step - loss: 0.5303 - acc: 0.7830\n",
            "Epoch 99/100\n",
            "41580/41580 [==============================] - 4s 93us/step - loss: 0.5267 - acc: 0.7847\n",
            "Epoch 100/100\n",
            "41580/41580 [==============================] - 4s 94us/step - loss: 0.5292 - acc: 0.7846\n",
            "17820/17820 [==============================] - 0s 26us/step\n",
            "\n",
            " ACCURACY: 77.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHgg_fDneiIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict and save results to csv file\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\n",
        "pred = model.predict(x_prob)\n",
        "features = [feature.replace(\"x0_\", \"\") for feature in y_oh_enc.get_feature_names()]\n",
        "results_index = np.argmax(pred, axis=1)\n",
        "results = []\n",
        "for r_idx in results_index:\n",
        "    results = results + [features[r_idx]]\n",
        "results = np.asarray(results)\n",
        "results = np.vstack((x_prob_id.T, results.T)).T\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['id', 'status_group'])\n",
        "results_df.to_csv('data.csv', index=False)\n",
        "!cp data.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alW8B7D8gDSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25534d49-9d7a-40d3-9f7d-48eeb1bb91e0"
      },
      "source": [
        ""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}